{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny few shot classifer\n",
    "\n",
    "Im folgenden wird ein *few shot classifier* für *multilabels* im [SetFit-Frame](https://huggingface.co/blog/setfit) Format erzeugt. Die *multilabels*-Option ist deshalb von Nöten, weil durchaus mehrere Future Skills in einem Kurs vermittelt werden können.\n",
    "\n",
    "Gearbeitet wird mit `Python 3.12.3`, in einem eigens dafür erzeugten Poetry Enviorment (`Poetry Future Skills Enviorment`). Das Enviorment wird als Kernel händisch in Jupyter Notebook geladen. \n",
    "\n",
    "Weitere Pakete können dem Enviorment mit `poetry add <paketname>` hinzugefügt werden:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pakete laden\n",
    "\n",
    "In einem ersten Schritt werden die nötigen Pakete geladen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset  # Zum Laden von Datensätzen aus der Hugging Face `datasets` Bibliothek\n",
    "from sentence_transformers.losses import CosineSimilarityLoss  # Verlustfunktion basierend auf der Kosinusähnlichkeit, für Aufgaben wie Textähnlichkeit\n",
    "\n",
    "from setfit import SetFitModel, SetFitTrainer  # Zum Laden des SetFit-Modells und Trainers, speziell für Few-Shot Learning mit Satztransformern\n",
    "import pandas as pd  # Pandas für Datenmanipulation und -analyse\n",
    "from sklearn.preprocessing import LabelEncoder  # Zum Encoden von Labels in numerische Werte für maschinelles Lernen\n",
    "import os  # Für den Umgang mit Betriebssystemfunktionen (z.B. Dateipfade)\n",
    "from datasets import Dataset, DatasetDict  # Zum Erstellen und Verwalten von Datensätzen in Hugging Face `datasets`-Format\n",
    "from sklearn.model_selection import train_test_split  # Zum Aufteilen der Daten in Trainings- und Testsets\n",
    "\n",
    "from transformers import TrainingArguments  # Zum Festlegen von Trainingsparametern für Modelle in der Transformers-Bibliothek\n",
    "\n",
    "import torch  # Ermöglicht die GPU-Nutzung und effiziente Verarbeitung von Tensoren\n",
    "import numpy as np  # Für numerische Berechnungen und Arbeiten mit Arrays\n",
    "\n",
    "import re  # Paket für reguläre Ausdrücke (Regular Expressions), zum Suchen, Ersetzen oder Validieren von Textmustern.\n",
    "import html  # Modul zum Umgang mit HTML, wie z.B. Entschlüsseln von HTML-Entities.\n",
    "from unidecode import unidecode  # Bibliothek zum Entfernen von diakritischen Zeichen (z.B. Akzente) und zum Normalisieren von Unicode-Zeichen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbeitsverzeichnis bestimmen\n",
    "\n",
    "Es wird weiterhin das Rootverzeichnis bestimmt um es später über relative Pfade Daten und Modelle laden zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mhu\\Documents\\github\\future_skill_classification\\Py\\notebooks\n"
     ]
    }
   ],
   "source": [
    "# checke home-verzeichnis\n",
    "print(os.getcwd())\n",
    "# setze korrektes Home-Verzeichnis, falls das derzeitige nicht das korrekte ist.\n",
    "\n",
    "# PRIVATRECHNER:\n",
    "#os.chdir('c:/Users/Hueck/OneDrive/Dokumente/GitHub/future_skill_classification/')\n",
    "\n",
    "# ARBEITSRECHNER:\n",
    "os.chdir('c:/Users/mhu/Documents/github/future_skill_classification/Py/notebooks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainings- und Testdaten\n",
    "\n",
    "Von Yannic Hinrichs wurden per String-Match Trainingsdaten erzeugt, die um weitere Daten ergänzt wurden, die keine Future Skills enthalten. Die Daten wurden weiterhin durch Yannic Hinrichs händisch kontrolliert. \n",
    "\n",
    "Alternativ bestehen weiterhin die Daten, die Franziska Weber für das Training ihres Classifiers verwendet hat. Dieses werden derzeit (Stand 21.10.24) ebenfalls für das Training des folgenden Classfiers verwendet.\n",
    "\n",
    "### Daten laden\n",
    "\n",
    "Testdaten liegen zu jetzigen Zeitpunkt nicht vor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train_data_franziska.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Laden der Trainingsdaten\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/train_data_franziska.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m (df\n\u001b[0;32m      5\u001b[0m       \u001b[38;5;241m.\u001b[39massign(sentence\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m))  \u001b[38;5;66;03m# sentence-Spalte als String deklarieren\u001b[39;00m\n\u001b[0;32m      6\u001b[0m       \u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# NAs durch 0 ersetzen\u001b[39;00m\n\u001b[0;32m      7\u001b[0m       \u001b[38;5;241m.\u001b[39mreplace([np\u001b[38;5;241m.\u001b[39minf, \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf], \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# infs durch 0 ersetzen\u001b[39;00m\n\u001b[0;32m      8\u001b[0m       \u001b[38;5;241m.\u001b[39mpipe(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39massign(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{col: x[col]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcolumns}))  \u001b[38;5;66;03m# Numerische Variablen runden\u001b[39;00m\n\u001b[0;32m      9\u001b[0m      )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Für die weitere Verarbeitung werden die Daten ins Hugging Face Dataset-Format transformiert. \u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# # /////////////////////////////////////////////////\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# # Hinweis: Das Dataset-Format der Hugging Face datasets-Bibliothek basiert auf Apache Arrow und ist speicher-effizient, was schnelles Laden und Verarbeiten großer Datensätze ermöglicht. Es integriert sich nahtlos in Machine-Learning-Workflows und unterstützt einfache Transformationen und Vorverarbeitung durch Methoden wie .map(). Zudem ist es flexibel und kompatibel mit Pandas DataFrames sowie gängigen Frameworks wie PyTorch und TensorFlow.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# #//////////////////////////////////////////////////\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1557\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1400\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1404\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1405\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1406\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train_data_franziska.xlsx'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Laden der Trainingsdaten\n",
    "df = pd.read_excel('data/train_data_franziska.xlsx')\n",
    "\n",
    "df = (df\n",
    "      .assign(sentence=lambda x: x['sentence'].astype(str))  # sentence-Spalte als String deklarieren\n",
    "      .fillna(0)  # NAs durch 0 ersetzen\n",
    "      .replace([np.inf, -np.inf], 0)  # infs durch 0 ersetzen\n",
    "      .pipe(lambda x: x.assign(**{col: x[col].astype('int64') for col in x.select_dtypes(include='number').columns}))  # Numerische Variablen runden\n",
    "     )\n",
    "\n",
    "# Für die weitere Verarbeitung werden die Daten ins Hugging Face Dataset-Format transformiert. \n",
    "\n",
    "# # /////////////////////////////////////////////////\n",
    "# # Hinweis: Das Dataset-Format der Hugging Face datasets-Bibliothek basiert auf Apache Arrow und ist speicher-effizient, was schnelles Laden und Verarbeiten großer Datensätze ermöglicht. Es integriert sich nahtlos in Machine-Learning-Workflows und unterstützt einfache Transformationen und Vorverarbeitung durch Methoden wie .map(). Zudem ist es flexibel und kompatibel mit Pandas DataFrames sowie gängigen Frameworks wie PyTorch und TensorFlow.\n",
    "# #//////////////////////////////////////////////////\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten verarbeiten\n",
    "\n",
    "#### Feature Auswahl\n",
    "\n",
    "Nun werden die Features des Classifiers ausgewählt. Diese werden von den Kurstitel und -beschreibungen (`sentence`) separiert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ziehe Column-Names als Liste\n",
    "features = dataset.column_names\n",
    "\n",
    "# Definiere Columns, die nicht vom classifier berücksichtigt werden\n",
    "to_remove = ['sentence','Digital Literacy', 'Digital Ethics', 'Digitale Kollaboration', 'Digital Learning', 'Agiles Arbeiten', 'Lösungsfähigkeit', 'Kreativität', 'Unternehmerisches Handeln & Eigeninitiative', 'Interkulturelle Kommunikation', 'Resilienz', 'Urteilsfähigkeit', 'Innovationskompetenz', 'Missionsorientierung', 'Veränderungskompetenz', 'Dialog- und Konfliktfähigkeit'] \n",
    "\n",
    "# Schließe die oben definierten Variablen aus der Feature-Liste aus\n",
    "filtered_features = [feature for feature in features if feature not in to_remove]\n",
    "\n",
    "features = filtered_features\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot-Encoding\n",
    "\n",
    "Für das Trainieren des Classifiers wird im folgenden ein *One-Hot-Encoding*-Array der Features erzeugt. Dies ist deshalb nötig, da im folgenden ein Modell für ein Multi-Label-Szenario trainiert werden soll: Ein Kursangebot kann den erwerb mehrerer Future Skills versprechen, etwa Softwareentwicklung **&** IT-Architektur. Das One-Hot-Encoding wird benötigt, um die Labels der Features in eine Form zu bringen, die das Classifier Modell von SetFit korrekt verarbeiten kann. Es stellt sicher, dass alle Klassen gleichwertig behandelt werden, und ermöglicht die Berechnung von Ähnlichkeiten verschiedener Kursinhalte.\n",
    "\n",
    "Um den *One-Hot-Encoding*-Array zu erzeugen definieren wir `encode_labels(record)`: Diese Funktion geht durch jede Zeile eines Data Frames und erstellt eine neue Spalte namens `labels`. In dieser Spalte werden die Werte aus den oben definierten \"features\" gesammelt und in eine Liste gepackt. Diese Liste enthält Werte der Features.\n",
    "\n",
    "Beispiel: Mit Blick auf die Features 'Data Analytics & KI', 'Softwareentwicklung' und 'Nutzerzentriertes Design', könnte ein Zeilenvektor des Arrays eines Kurses der sich mit Data Analytics & KI / Softwareentwicklung beschäftigt so aussehen: `[1, 1, 0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generiere Funktion für One-Hot-Encoding für jede Zeile (record)\n",
    "def encode_labels(record):\n",
    "    return {\"labels\": [record[feature] for feature in features]}\n",
    "\n",
    "# Wende Funktion auf train_data an\n",
    "dataset = dataset.map(encode_labels)\n",
    "\n",
    "# Um die Daten übersichtlich im Viewer zu betrachten: Umwandlung und Pandas Data Frame\n",
    "pd_dataset = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir kopieren als nächstes das `dataset`-Objekt in ein neues Objekt namens `train_data` und ziehen davon nochmals eine Kopie mit Pandas, um die Daten ein letztes Mal im Viewer zu prüfen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset\n",
    "\n",
    "# check data with pandas\n",
    "pd_train_dataset = train_dataset.to_pandas() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelltraining\n",
    "\n",
    "### Vorbereitung des Modells\n",
    "\n",
    "Es wird das vortrainierte sentence-transformers-Modell `paraphrase-multilingual-MiniLM-L12-v2` aus der Hugging Face Bibliothek geladen. Durch die Angabe der Strategie `one-vs-rest` wird das Modell für eine Multi-Label-Klassifikation konfiguriert, bei der jede Klasse eines Kurses einzeln vorhergesagt wird. Auf diese Weise wird es möglich, mehrere Skills pro Kurs zu identifizieren.\n",
    "\n",
    "Das Modell `paraphrase-multilingual-MiniLM-L12-v2` eignet sich gut zur Klassifikation von Future Skills, da es auf semantische Ähnlichkeit und Paraphrasierung trainiert wurde. Dadurch kann es unterschiedliche Formulierungen eines Skills als denselben Future Skill erkennen, was bei der Erfassung von Konzepten hilfreich ist, die oft unterschiedlich beschrieben werden. Seine Multilingualität macht es zudem flexibel für Kursdaten in verschiedenen Sprachen. Dank der MiniLM-Architektur ist das Modell effizient und ermöglicht schnelle Vorhersagen, ohne dabei an Leistungsfähigkeit zu verlieren. Diese Eigenschaften machen es besonders nützlich, um abstrakte oder neu formulierte Skills zuverlässig zu klassifizieren.\n",
    "\n",
    "Der Parameter `multi_target_strategy=\"multi-output\"` ist eine spezifische Strategie, die für Multilabel-Klassifikationen verwendet wird, bei denen jedes Label als unabhängiger Klassifikationsausgang behandelt wird. Das bedeutet, dass das Modell für jeden Future Skill eine eigene, separate Vorhersage trifft, anstatt alle Labels als zusammenhängend zu betrachten. Diese Strategie ist besonders nützlich, wenn es keine Abhängigkeit zwischen den Labels gibt und jeder Future Skill unabhängig von den anderen erlernt oder vorhergesagt werden kann. Dadurch kann das Modell präziser und flexibler für komplexe Klassifikationsaufgaben mit mehreren Zielen arbeiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden des Modells\n",
    "model = SetFitModel.from_pretrained(\n",
    "    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", \n",
    "    multi_target_strategy=\"one-vs-rest\",\n",
    "    labels=['Data Analytics & KI', 'Softwareentwicklung', 'Nutzerzentriertes Design', 'IT-Architektur', 'Hardware/Robotikentwicklung', 'Quantencomputing'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auswahl GPU/CPU\n",
    "\n",
    "Es wird zunächst geprüft, ob CUDA (und damit eine GPU) verfügbar ist, indem `torch.cuda.is_available()` aufgerufen wird. Wenn CUDA verfügbar ist, wird die GPU als Rechengerät (`device`) ausgewählt, und der Name der GPU mit `torch.cuda.get_device_name(0)` ausgegeben. Falls CUDA nicht verfügbar ist, wird stattdessen die CPU als Rechengerät festgelegt. Anschließend wird das Modell mit `model.to(device)` entweder auf die GPU oder die CPU verschoben, abhängig von der zuvor getroffenen Geräteauswahl, um die Berechnungen entsprechend durchzuführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prüfen, ob CUDA (GPU) verfügbar ist\n",
    "print(\"CUDA verfügbar:\", torch.cuda.is_available())\n",
    "\n",
    "# Wenn CUDA verfügbar ist, die GPU verwenden\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Verwende CPU\")\n",
    "\n",
    "# model auf GPU verschieben\n",
    "\n",
    "    # Sicherstellen, dass das Modell auf der GPU ist, wenn verfügbar\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für den Fall dass eine GPU vorhanden ist: `PyTorch` wird verwendet, um die GPU für das Training von Modellen zu verwenden. `PyTorch` nutzt einen Speicher-Caching-Mechanismus, um GPU-Speicher effizient zu verwalten. Wenn Tensoren gelöscht oder nicht mehr benötigt werden, bleibt der Speicher oft noch im Cache, damit zukünftige Speicheranforderungen schneller bedient werden können. Der Befehl `torch.cuda.empty_cache()` leert im folgenden den Cache und gibt den belegten Speicher an das GPU-System zurück, ohne jedoch den aktuell in Verwendung befindlichen Speicher zu beeinflussen. Wird der Cache nicht regelmäßiig geleert, kann das einen ernormen Einfluss auf die Bearbeitungszeit des Trainings eines Classifiers haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der `SetFitTrainer` wird instanziiert, um das Modell auf den Trainings- und Evaluierungsdatensätzen zu trainieren. Eine Bewertung des Modells wird erst zu einem späteren Zeitpunkt möglich, wenn Testdaten vorliegen.\n",
    "\n",
    "1. Das vortrainierte `model` (siehe oben) wird als Grundlage für das Training verwendet, das auf dem Few-Shot Learning-Ansatz basiert.\n",
    "2. Der `train_dataset` enthält die Trainingsdaten, ein `eval_dataset` liegt derzeit nicht vor.\n",
    "3. Die Verlustfunktion (`loss_class`) wird als *Cosine Similarity Loss* festgelegt. Die *Cosine Similarity Loss* ist eine Verlustfunktion, die darauf basiert, wie ähnlich sich zwei Vektoren sind. In diesem Fall werden die Ausgabevektoren des Modells und die Zielvektoren (also die \"Labels\") mithilfe der Kosinus-Ähnlichkeit verglichen. Je höher die Ähnlichkeit also, desto besser das Modell. \n",
    "4. Die `column_mapping` gibt an, welche Spalten in den Datensätzen die Texte (`sentence` → `text`) und die zugehörigen Labels (`labels` → `label`) enthalten, sodass das Modell die Daten korrekt interpretieren kann.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset, \n",
    "    #eval_dataset=eval_dataset,\n",
    "    loss_class=CosineSimilarityLoss,\n",
    "    num_iterations=40, \n",
    "    num_epochs=2,\n",
    "    column_mapping={\"sentence\": \"text\", \"labels\": \"label\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun kann das Training beginnen: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das nach-trainierte Modell wird in einem ersten Schritt lokal gespeichert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speichern des Modells auf deinem PC\n",
    "model.save_pretrained(\"models\")  # Ersetze den Pfad durch deinen gewünschten Speicherort\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In einem weiteren Schritt wird das Modeel auf das Hugging-Face-Hub geladen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SetFitModel.from_pretrained(\"models\")  # Ändere den Pfad zu deinem Modell\n",
    "\n",
    "trainer.push_to_hub(\"Chernoffface/fs-setfit-multilable-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done. Das Training ist beendet 🤗."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fs_skills_classifier_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
