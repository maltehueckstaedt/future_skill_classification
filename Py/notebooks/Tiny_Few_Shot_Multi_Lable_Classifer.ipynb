{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny few shot classifer\n",
    "\n",
    "Im folgenden wird ein *few shot classifier* f√ºr *multilabels* im [SetFit-Frame](https://huggingface.co/blog/setfit) Format erzeugt. Die *multilabels*-Option ist deshalb von N√∂ten, weil durchaus mehrere Future Skills in einem Kurs vermittelt werden k√∂nnen.\n",
    "\n",
    "Gearbeitet wird mit `Python 3.12.3`, in einem eigens daf√ºr erzeugten Poetry Enviorment (`Poetry Future Skills Enviorment`). Das Enviorment wird als Kernel h√§ndisch in Jupyter Notebook geladen. \n",
    "\n",
    "Weitere Pakete k√∂nnen dem Enviorment mit `poetry add <paketname>` hinzugef√ºgt werden:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pakete laden\n",
    "\n",
    "In einem ersten Schritt werden die n√∂tigen Pakete geladen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset  # Zum Laden von Datens√§tzen aus der Hugging Face `datasets` Bibliothek\n",
    "from sentence_transformers.losses import CosineSimilarityLoss  # Verlustfunktion basierend auf der Kosinus√§hnlichkeit, f√ºr Aufgaben wie Text√§hnlichkeit\n",
    "\n",
    "from setfit import SetFitModel, SetFitTrainer  # Zum Laden des SetFit-Modells und Trainers, speziell f√ºr Few-Shot Learning mit Satztransformern\n",
    "import pandas as pd  # Pandas f√ºr Datenmanipulation und -analyse\n",
    "from sklearn.preprocessing import LabelEncoder  # Zum Encoden von Labels in numerische Werte f√ºr maschinelles Lernen\n",
    "import os  # F√ºr den Umgang mit Betriebssystemfunktionen (z.B. Dateipfade)\n",
    "from datasets import Dataset, DatasetDict  # Zum Erstellen und Verwalten von Datens√§tzen in Hugging Face `datasets`-Format\n",
    "from sklearn.model_selection import train_test_split  # Zum Aufteilen der Daten in Trainings- und Testsets\n",
    "\n",
    "from transformers import TrainingArguments  # Zum Festlegen von Trainingsparametern f√ºr Modelle in der Transformers-Bibliothek\n",
    "\n",
    "import torch  # Erm√∂glicht die GPU-Nutzung und effiziente Verarbeitung von Tensoren\n",
    "import numpy as np  # F√ºr numerische Berechnungen und Arbeiten mit Arrays\n",
    "\n",
    "import re  # Paket f√ºr regul√§re Ausdr√ºcke (Regular Expressions), zum Suchen, Ersetzen oder Validieren von Textmustern.\n",
    "import html  # Modul zum Umgang mit HTML, wie z.B. Entschl√ºsseln von HTML-Entities.\n",
    "from unidecode import unidecode  # Bibliothek zum Entfernen von diakritischen Zeichen (z.B. Akzente) und zum Normalisieren von Unicode-Zeichen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbeitsverzeichnis bestimmen\n",
    "\n",
    "Es wird weiterhin das Rootverzeichnis bestimmt um es sp√§ter √ºber relative Pfade Daten und Modelle laden zu k√∂nnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mhu\\Documents\\github\\future_skill_classification\\Py\\notebooks\n"
     ]
    }
   ],
   "source": [
    "# checke home-verzeichnis\n",
    "print(os.getcwd())\n",
    "# setze korrektes Home-Verzeichnis, falls das derzeitige nicht das korrekte ist.\n",
    "\n",
    "# PRIVATRECHNER:\n",
    "#os.chdir('c:/Users/Hueck/OneDrive/Dokumente/GitHub/future_skill_classification/')\n",
    "\n",
    "# ARBEITSRECHNER:\n",
    "os.chdir('c:/Users/mhu/Documents/github/future_skill_classification/Py/notebooks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainings- und Testdaten\n",
    "\n",
    "Von Yannic Hinrichs wurden per String-Match Trainingsdaten erzeugt, die um weitere Daten erg√§nzt wurden, die keine Future Skills enthalten. Die Daten wurden weiterhin durch Yannic Hinrichs h√§ndisch kontrolliert. \n",
    "\n",
    "Alternativ bestehen weiterhin die Daten, die Franziska Weber f√ºr das Training ihres Classifiers verwendet hat. Dieses werden derzeit (Stand 21.10.24) ebenfalls f√ºr das Training des folgenden Classfiers verwendet.\n",
    "\n",
    "### Daten laden\n",
    "\n",
    "Testdaten liegen zu jetzigen Zeitpunkt nicht vor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train_data_franziska.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Laden der Trainingsdaten\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/train_data_franziska.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m (df\n\u001b[0;32m      5\u001b[0m       \u001b[38;5;241m.\u001b[39massign(sentence\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m))  \u001b[38;5;66;03m# sentence-Spalte als String deklarieren\u001b[39;00m\n\u001b[0;32m      6\u001b[0m       \u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# NAs durch 0 ersetzen\u001b[39;00m\n\u001b[0;32m      7\u001b[0m       \u001b[38;5;241m.\u001b[39mreplace([np\u001b[38;5;241m.\u001b[39minf, \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf], \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# infs durch 0 ersetzen\u001b[39;00m\n\u001b[0;32m      8\u001b[0m       \u001b[38;5;241m.\u001b[39mpipe(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39massign(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{col: x[col]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcolumns}))  \u001b[38;5;66;03m# Numerische Variablen runden\u001b[39;00m\n\u001b[0;32m      9\u001b[0m      )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# F√ºr die weitere Verarbeitung werden die Daten ins Hugging Face Dataset-Format transformiert. \u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# # /////////////////////////////////////////////////\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# # Hinweis: Das Dataset-Format der Hugging Face datasets-Bibliothek basiert auf Apache Arrow und ist speicher-effizient, was schnelles Laden und Verarbeiten gro√üer Datens√§tze erm√∂glicht. Es integriert sich nahtlos in Machine-Learning-Workflows und unterst√ºtzt einfache Transformationen und Vorverarbeitung durch Methoden wie .map(). Zudem ist es flexibel und kompatibel mit Pandas DataFrames sowie g√§ngigen Frameworks wie PyTorch und TensorFlow.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# #//////////////////////////////////////////////////\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1557\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1400\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1404\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1405\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1406\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train_data_franziska.xlsx'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Laden der Trainingsdaten\n",
    "df = pd.read_excel('data/train_data_franziska.xlsx')\n",
    "\n",
    "df = (df\n",
    "      .assign(sentence=lambda x: x['sentence'].astype(str))  # sentence-Spalte als String deklarieren\n",
    "      .fillna(0)  # NAs durch 0 ersetzen\n",
    "      .replace([np.inf, -np.inf], 0)  # infs durch 0 ersetzen\n",
    "      .pipe(lambda x: x.assign(**{col: x[col].astype('int64') for col in x.select_dtypes(include='number').columns}))  # Numerische Variablen runden\n",
    "     )\n",
    "\n",
    "# F√ºr die weitere Verarbeitung werden die Daten ins Hugging Face Dataset-Format transformiert. \n",
    "\n",
    "# # /////////////////////////////////////////////////\n",
    "# # Hinweis: Das Dataset-Format der Hugging Face datasets-Bibliothek basiert auf Apache Arrow und ist speicher-effizient, was schnelles Laden und Verarbeiten gro√üer Datens√§tze erm√∂glicht. Es integriert sich nahtlos in Machine-Learning-Workflows und unterst√ºtzt einfache Transformationen und Vorverarbeitung durch Methoden wie .map(). Zudem ist es flexibel und kompatibel mit Pandas DataFrames sowie g√§ngigen Frameworks wie PyTorch und TensorFlow.\n",
    "# #//////////////////////////////////////////////////\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten verarbeiten\n",
    "\n",
    "#### Feature Auswahl\n",
    "\n",
    "Nun werden die Features des Classifiers ausgew√§hlt. Diese werden von den Kurstitel und -beschreibungen (`sentence`) separiert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ziehe Column-Names als Liste\n",
    "features = dataset.column_names\n",
    "\n",
    "# Definiere Columns, die nicht vom classifier ber√ºcksichtigt werden\n",
    "to_remove = ['sentence','Digital Literacy', 'Digital Ethics', 'Digitale Kollaboration', 'Digital Learning', 'Agiles Arbeiten', 'L√∂sungsf√§higkeit', 'Kreativit√§t', 'Unternehmerisches Handeln & Eigeninitiative', 'Interkulturelle Kommunikation', 'Resilienz', 'Urteilsf√§higkeit', 'Innovationskompetenz', 'Missionsorientierung', 'Ver√§nderungskompetenz', 'Dialog- und Konfliktf√§higkeit'] \n",
    "\n",
    "# Schlie√üe die oben definierten Variablen aus der Feature-Liste aus\n",
    "filtered_features = [feature for feature in features if feature not in to_remove]\n",
    "\n",
    "features = filtered_features\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot-Encoding\n",
    "\n",
    "F√ºr das Trainieren des Classifiers wird im folgenden ein *One-Hot-Encoding*-Array der Features erzeugt. Dies ist deshalb n√∂tig, da im folgenden ein Modell f√ºr ein Multi-Label-Szenario trainiert werden soll: Ein Kursangebot kann den erwerb mehrerer Future Skills versprechen, etwa Softwareentwicklung **&** IT-Architektur. Das One-Hot-Encoding wird ben√∂tigt, um die Labels der Features in eine Form zu bringen, die das Classifier Modell von SetFit korrekt verarbeiten kann. Es stellt sicher, dass alle Klassen gleichwertig behandelt werden, und erm√∂glicht die Berechnung von √Ñhnlichkeiten verschiedener Kursinhalte.\n",
    "\n",
    "Um den *One-Hot-Encoding*-Array zu erzeugen definieren wir `encode_labels(record)`: Diese Funktion geht durch jede Zeile eines Data Frames und erstellt eine neue Spalte namens `labels`. In dieser Spalte werden die Werte aus den oben definierten \"features\" gesammelt und in eine Liste gepackt. Diese Liste enth√§lt Werte der Features.\n",
    "\n",
    "Beispiel: Mit Blick auf die Features 'Data Analytics & KI', 'Softwareentwicklung' und 'Nutzerzentriertes Design', k√∂nnte ein Zeilenvektor des Arrays eines Kurses der sich mit Data Analytics & KI / Softwareentwicklung besch√§ftigt so aussehen: `[1, 1, 0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generiere Funktion f√ºr One-Hot-Encoding f√ºr jede Zeile (record)\n",
    "def encode_labels(record):\n",
    "    return {\"labels\": [record[feature] for feature in features]}\n",
    "\n",
    "# Wende Funktion auf train_data an\n",
    "dataset = dataset.map(encode_labels)\n",
    "\n",
    "# Um die Daten √ºbersichtlich im Viewer zu betrachten: Umwandlung und Pandas Data Frame\n",
    "pd_dataset = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir kopieren als n√§chstes das `dataset`-Objekt in ein neues Objekt namens `train_data` und ziehen davon nochmals eine Kopie mit Pandas, um die Daten ein letztes Mal im Viewer zu pr√ºfen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset\n",
    "\n",
    "# check data with pandas\n",
    "pd_train_dataset = train_dataset.to_pandas() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelltraining\n",
    "\n",
    "### Vorbereitung des Modells\n",
    "\n",
    "Es wird das vortrainierte sentence-transformers-Modell `paraphrase-multilingual-MiniLM-L12-v2` aus der Hugging Face Bibliothek geladen. Durch die Angabe der Strategie `one-vs-rest` wird das Modell f√ºr eine Multi-Label-Klassifikation konfiguriert, bei der jede Klasse eines Kurses einzeln vorhergesagt wird. Auf diese Weise wird es m√∂glich, mehrere Skills pro Kurs zu identifizieren.\n",
    "\n",
    "Das Modell `paraphrase-multilingual-MiniLM-L12-v2` eignet sich gut zur Klassifikation von Future Skills, da es auf semantische √Ñhnlichkeit und Paraphrasierung trainiert wurde. Dadurch kann es unterschiedliche Formulierungen eines Skills als denselben Future Skill erkennen, was bei der Erfassung von Konzepten hilfreich ist, die oft unterschiedlich beschrieben werden. Seine Multilingualit√§t macht es zudem flexibel f√ºr Kursdaten in verschiedenen Sprachen. Dank der MiniLM-Architektur ist das Modell effizient und erm√∂glicht schnelle Vorhersagen, ohne dabei an Leistungsf√§higkeit zu verlieren. Diese Eigenschaften machen es besonders n√ºtzlich, um abstrakte oder neu formulierte Skills zuverl√§ssig zu klassifizieren.\n",
    "\n",
    "Der Parameter `multi_target_strategy=\"multi-output\"` ist eine spezifische Strategie, die f√ºr Multilabel-Klassifikationen verwendet wird, bei denen jedes Label als unabh√§ngiger Klassifikationsausgang behandelt wird. Das bedeutet, dass das Modell f√ºr jeden Future Skill eine eigene, separate Vorhersage trifft, anstatt alle Labels als zusammenh√§ngend zu betrachten. Diese Strategie ist besonders n√ºtzlich, wenn es keine Abh√§ngigkeit zwischen den Labels gibt und jeder Future Skill unabh√§ngig von den anderen erlernt oder vorhergesagt werden kann. Dadurch kann das Modell pr√§ziser und flexibler f√ºr komplexe Klassifikationsaufgaben mit mehreren Zielen arbeiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden des Modells\n",
    "model = SetFitModel.from_pretrained(\n",
    "    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", \n",
    "    multi_target_strategy=\"one-vs-rest\",\n",
    "    labels=['Data Analytics & KI', 'Softwareentwicklung', 'Nutzerzentriertes Design', 'IT-Architektur', 'Hardware/Robotikentwicklung', 'Quantencomputing'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auswahl GPU/CPU\n",
    "\n",
    "Es wird zun√§chst gepr√ºft, ob CUDA (und damit eine GPU) verf√ºgbar ist, indem `torch.cuda.is_available()` aufgerufen wird. Wenn CUDA verf√ºgbar ist, wird die GPU als Rechenger√§t (`device`) ausgew√§hlt, und der Name der GPU mit `torch.cuda.get_device_name(0)` ausgegeben. Falls CUDA nicht verf√ºgbar ist, wird stattdessen die CPU als Rechenger√§t festgelegt. Anschlie√üend wird das Modell mit `model.to(device)` entweder auf die GPU oder die CPU verschoben, abh√§ngig von der zuvor getroffenen Ger√§teauswahl, um die Berechnungen entsprechend durchzuf√ºhren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√ºfen, ob CUDA (GPU) verf√ºgbar ist\n",
    "print(\"CUDA verf√ºgbar:\", torch.cuda.is_available())\n",
    "\n",
    "# Wenn CUDA verf√ºgbar ist, die GPU verwenden\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Verwende CPU\")\n",
    "\n",
    "# model auf GPU verschieben\n",
    "\n",
    "    # Sicherstellen, dass das Modell auf der GPU ist, wenn verf√ºgbar\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F√ºr den Fall dass eine GPU vorhanden ist: `PyTorch` wird verwendet, um die GPU f√ºr das Training von Modellen zu verwenden. `PyTorch` nutzt einen Speicher-Caching-Mechanismus, um GPU-Speicher effizient zu verwalten. Wenn Tensoren gel√∂scht oder nicht mehr ben√∂tigt werden, bleibt der Speicher oft noch im Cache, damit zuk√ºnftige Speicheranforderungen schneller bedient werden k√∂nnen. Der Befehl `torch.cuda.empty_cache()` leert im folgenden den Cache und gibt den belegten Speicher an das GPU-System zur√ºck, ohne jedoch den aktuell in Verwendung befindlichen Speicher zu beeinflussen. Wird der Cache nicht regelm√§√üiig geleert, kann das einen ernormen Einfluss auf die Bearbeitungszeit des Trainings eines Classifiers haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der `SetFitTrainer` wird instanziiert, um das Modell auf den Trainings- und Evaluierungsdatens√§tzen zu trainieren. Eine Bewertung des Modells wird erst zu einem sp√§teren Zeitpunkt m√∂glich, wenn Testdaten vorliegen.\n",
    "\n",
    "1. Das vortrainierte `model` (siehe oben) wird als Grundlage f√ºr das Training verwendet, das auf dem Few-Shot Learning-Ansatz basiert.\n",
    "2. Der `train_dataset` enth√§lt die Trainingsdaten, ein `eval_dataset` liegt derzeit nicht vor.\n",
    "3. Die Verlustfunktion (`loss_class`) wird als *Cosine Similarity Loss* festgelegt. Die *Cosine Similarity Loss* ist eine Verlustfunktion, die darauf basiert, wie √§hnlich sich zwei Vektoren sind. In diesem Fall werden die Ausgabevektoren des Modells und die Zielvektoren (also die \"Labels\") mithilfe der Kosinus-√Ñhnlichkeit verglichen. Je h√∂her die √Ñhnlichkeit also, desto besser das Modell. \n",
    "4. Die `column_mapping` gibt an, welche Spalten in den Datens√§tzen die Texte (`sentence` ‚Üí `text`) und die zugeh√∂rigen Labels (`labels` ‚Üí `label`) enthalten, sodass das Modell die Daten korrekt interpretieren kann.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset, \n",
    "    #eval_dataset=eval_dataset,\n",
    "    loss_class=CosineSimilarityLoss,\n",
    "    num_iterations=40, \n",
    "    num_epochs=2,\n",
    "    column_mapping={\"sentence\": \"text\", \"labels\": \"label\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun kann das Training beginnen: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das nach-trainierte Modell wird in einem ersten Schritt lokal gespeichert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speichern des Modells auf deinem PC\n",
    "model.save_pretrained(\"models\")  # Ersetze den Pfad durch deinen gew√ºnschten Speicherort\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In einem weiteren Schritt wird das Modeel auf das Hugging-Face-Hub geladen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SetFitModel.from_pretrained(\"models\")  # √Ñndere den Pfad zu deinem Modell\n",
    "\n",
    "trainer.push_to_hub(\"Chernoffface/fs-setfit-multilable-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done. Das Training ist beendet ü§ó."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fs_skills_classifier_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
