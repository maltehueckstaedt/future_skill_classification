{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny few shot classifer\n",
    "\n",
    "Im folgenden wird ein *few shot classifier* für *multilabels* im [SetFit-Frame](https://huggingface.co/blog/setfit) Format erzeugt. Die *multilabels*-Option ist deshalb von Nöten, weil durchaus mehrere Future Skills in einem Kurs vermittelt werden können.\n",
    "\n",
    "Gearbeitet wird mit `Python 3.12.3`, in einem eigens dafür erzeugten Poetry Enviorment (`Poetry Future Skills Enviorment`). Das Enviorment wird als Kernel händisch in Jupyter Notebook geladen. \n",
    "\n",
    "Weitere Pakete können dem Enviorment mit `poetry add <paketname>` hinzugefügt werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !poetry add platformdirs\n",
    "# !poetry add pyreadr\n",
    "# Abhänggkeiten anzeigen:\n",
    "# !poetry show --tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pakete laden\n",
    "\n",
    "In einem ersten Schritt werden die nötigen Pakete geladen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset  # Zum Laden von Datensätzen aus der Hugging Face `datasets` Bibliothek\n",
    "from sentence_transformers.losses import CosineSimilarityLoss  # Verlustfunktion basierend auf der Kosinusähnlichkeit, für Aufgaben wie Textähnlichkeit\n",
    "\n",
    "from setfit import SetFitModel, SetFitTrainer  # Zum Laden des SetFit-Modells und Trainers, speziell für Few-Shot Learning mit Satztransformern\n",
    "import pandas as pd  # Pandas für Datenmanipulation und -analyse\n",
    "from sklearn.preprocessing import LabelEncoder  # Zum Encoden von Labels in numerische Werte für maschinelles Lernen\n",
    "import os  # Für den Umgang mit Betriebssystemfunktionen (z.B. Dateipfade)\n",
    "from datasets import Dataset, DatasetDict  # Zum Erstellen und Verwalten von Datensätzen in Hugging Face `datasets`-Format\n",
    "from sklearn.model_selection import train_test_split  # Zum Aufteilen der Daten in Trainings- und Testsets\n",
    "\n",
    "from transformers import TrainingArguments  # Zum Festlegen von Trainingsparametern für Modelle in der Transformers-Bibliothek\n",
    "\n",
    "import torch  # Ermöglicht die GPU-Nutzung und effiziente Verarbeitung von Tensoren\n",
    "import numpy as np  # Für numerische Berechnungen und Arbeiten mit Arrays\n",
    "\n",
    "import re  # Paket für reguläre Ausdrücke (Regular Expressions), zum Suchen, Ersetzen oder Validieren von Textmustern.\n",
    "import html  # Modul zum Umgang mit HTML, wie z.B. Entschlüsseln von HTML-Entities.\n",
    "from unidecode import unidecode  # Bibliothek zum Entfernen von diakritischen Zeichen (z.B. Akzente) und zum Normalisieren von Unicode-Zeichen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbeitsverzeichnis bestimmen\n",
    "\n",
    "Es wird weiterhin das Rootverzeichnis bestimmt um es später über relative Pfade Daten und Modelle laden zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hueck\\OneDrive\\Dokumente\\GitHub\\future_skill_classification\\Py\n"
     ]
    }
   ],
   "source": [
    "# checke home-verzeichnis\n",
    "print(os.getcwd())\n",
    "# setze korrektes Home-Verzeichnis, falls das derzeitige nicht das korrekte ist.\n",
    "os.chdir('c:/Users/Hueck/OneDrive/Dokumente/GitHub/future_skill_classification/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainings- und Testdaten\n",
    "\n",
    "Von Yannic Hinrichs wurden per String-Match Trainingsdaten erzeugt, die um weitere Daten ergänzt wurden, die keine Future Skills enthalten. Die Daten wurden weiterhin durch Yannic Hinrichs händisch kontrolliert. Alternativ bestehen weiterhin die Daten, die Franziska Weber für das Training ihres Classifiers verwendet hat. Dieses werden derzeit (Stand 21.10.24) ebenfalls für das Training des folgenden Classfiers verwendet.\n",
    "\n",
    "### Daten laden\n",
    "\n",
    "Testdaten liegen zu jetzigen Zeitpunkt nicht vor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence', 'Data Analytics & KI', 'Softwareentwicklung', 'Nutzerzentriertes Design', 'IT-Architektur', 'Hardware/Robotikentwicklung', 'Quantencomputing', 'Digital Literacy', 'Digital Ethics', 'Digitale Kollaboration', 'Digital Learning', 'Agiles Arbeiten', 'Lösungsfähigkeit', 'Kreativität', 'Unternehmerisches Handeln & Eigeninitiative', 'Interkulturelle Kommunikation', 'Resilienz', 'Urteilsfähigkeit', 'Innovationskompetenz', 'Missionsorientierung', 'Veränderungskompetenz', 'Dialog- und Konfliktfähigkeit'],\n",
      "    num_rows: 1577\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Laden der Trainingsdaten\n",
    "df = pd.read_excel('data/train_data_franziska.xlsx')\n",
    "\n",
    "df = (df\n",
    "      .assign(sentence=lambda x: x['sentence'].astype(str))  # sentence-Spalte als String deklarieren\n",
    "      .fillna(0)  # NAs durch 0 ersetzen\n",
    "      .replace([np.inf, -np.inf], 0)  # infs durch 0 ersetzen\n",
    "      .pipe(lambda x: x.assign(**{col: x[col].astype('int64') for col in x.select_dtypes(include='number').columns}))  # Numerische Variablen runden\n",
    "     )\n",
    "\n",
    "# Für die weitere Verarbeitung werden die Daten ins Hugging Face Dataset-Format transformiert. \n",
    "\n",
    "# # /////////////////////////////////////////////////\n",
    "# # Hinweis: Das Dataset-Format der Hugging Face datasets-Bibliothek basiert auf Apache Arrow und ist speichereffizient, was schnelles Laden und Verarbeiten großer Datensätze ermöglicht. Es integriert sich nahtlos in Machine-Learning-Workflows und unterstützt einfache Transformationen und Vorverarbeitung durch Methoden wie .map(). Zudem ist es flexibel und kompatibel mit Pandas DataFrames sowie gängigen Frameworks wie PyTorch und TensorFlow.\n",
    "# #//////////////////////////////////////////////////\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten verarbeiten\n",
    "\n",
    "#### Feature Auswahl\n",
    "\n",
    "Nun werden die Features des Classifiers ausgewählt. Diese werden von den Kurstitel und -beschreibungen (`sentence`) separiert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data Analytics & KI', 'Softwareentwicklung', 'Nutzerzentriertes Design', 'IT-Architektur', 'Hardware/Robotikentwicklung', 'Quantencomputing']\n"
     ]
    }
   ],
   "source": [
    "# Ziehe Column-Names als Liste\n",
    "features = dataset.column_names\n",
    "\n",
    "# Definiere Columns, die nicht vom classifier berücksichtigt werden\n",
    "to_remove = ['sentence','Digital Literacy', 'Digital Ethics', 'Digitale Kollaboration', 'Digital Learning', 'Agiles Arbeiten', 'Lösungsfähigkeit', 'Kreativität', 'Unternehmerisches Handeln & Eigeninitiative', 'Interkulturelle Kommunikation', 'Resilienz', 'Urteilsfähigkeit', 'Innovationskompetenz', 'Missionsorientierung', 'Veränderungskompetenz', 'Dialog- und Konfliktfähigkeit'] \n",
    "\n",
    "# Schließe die oben definierten Variablen aus der Feature-Liste aus\n",
    "filtered_features = [feature for feature in features if feature not in to_remove]\n",
    "\n",
    "features = filtered_features\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot-Encoding\n",
    "\n",
    "Für das Trainieren des Classifiers wird im folgenden ein *One-Hot-Encoding*-Array der Features erzeugt. Dies ist deshalb nötig, da im folgenden ein Modell für ein Multi-Label-Szenario trainiert werden soll: Ein Kursangebot kann den erwerb mehrerer Future Skills versprechen, etwa Softwareentwicklung **&** IT-Architektur. Das One-Hot-Encoding wird benötigt, um die Labels der Features in eine Form zu bringen, die das Classifier Modell von SetFit korrekt verarbeiten kann. Es stellt sicher, dass alle Klassen gleichwertig behandelt werden, und ermöglicht die Berechnung von Ähnlichkeiten verschiedener Kursinhalte.\n",
    "\n",
    "Um den *One-Hot-Encoding*-Array zu erzeugen definieren wir `encode_labels(record)`: Diese Funktion geht durch jede Zeile eines Data Frames und erstellt eine neue Spalte namens `labels`. In dieser Spalte werden die Werte aus den oben definierten \"features\" gesammelt und in eine Liste gepackt. Diese Liste enthält Werte der Features.\n",
    "\n",
    "Beispiel: Mit Blick auf die Features 'Data Analytics & KI', 'Softwareentwicklung' und 'Nutzerzentriertes Design', könnte ein Zeilenvektor des Arrays eines Kurses der sich mit Data Analytics & KI / Softwareentwicklung beschäftigt so aussehen: `[1, 1, 0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hueck\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\future-skill-classification--ys6-NbF-py3.12\\Lib\\site-packages\\datasets\\utils\\_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\n",
      "  obj.co_lnotab,  # for < python 3.10 [not counted in args]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bcb5d6cd37f467c8aa63e4366a14eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1577 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generiere Funktion für One-Hot-Encoding für jede Zeile (record)\n",
    "def encode_labels(record):\n",
    "    return {\"labels\": [record[feature] for feature in features]}\n",
    "\n",
    "# Wende Funktion auf train_data an\n",
    "dataset = dataset.map(encode_labels)\n",
    "\n",
    "# Um die Daten übersichtlich im Viewer zu betrachten: Umwandlung und Pandas Data Frame\n",
    "pd_dataset = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir kopieren als nächstes das `dataset`-Objekt in ein neues Objekt namens `train_data` und ziehen davon nochmals eine Kopie mit Pandas, um die Daten ein letztes Mal im Viewer zu prüfen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset\n",
    "\n",
    "# check data with pandas\n",
    "pd_train_dataset = train_dataset.to_pandas() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelltraining\n",
    "\n",
    "### Vorbereitung des Modells\n",
    "\n",
    "Es wird das vortrainierte sentence-transformers-Modell `paraphrase-multilingual-MiniLM-L12-v2` aus der Hugging Face Bibliothek geladen. Durch die Angabe der Strategie `one-vs-rest` wird das Modell für eine Multi-Label-Klassifikation konfiguriert, bei der jede Klasse eines Kurses einzeln vorhergesagt wird. Auf diese Weise wird es möglich, mehrere Skills pro Kurs zu identifizieren.\n",
    "\n",
    "Das Modell `paraphrase-multilingual-MiniLM-L12-v2` eignet sich gut zur Klassifikation von Future Skills, da es auf semantische Ähnlichkeit und Paraphrasierung trainiert wurde. Dadurch kann es unterschiedliche Formulierungen eines Skills als denselben Future Skill erkennen, was bei der Erfassung von Konzepten hilfreich ist, die oft unterschiedlich beschrieben werden. Seine Multilingualität macht es zudem flexibel für Kursdaten in verschiedenen Sprachen. Dank der MiniLM-Architektur ist das Modell effizient und ermöglicht schnelle Vorhersagen, ohne dabei an Leistungsfähigkeit zu verlieren. Diese Eigenschaften machen es besonders nützlich, um abstrakte oder neu formulierte Skills zuverlässig zu klassifizieren.\n",
    "\n",
    "Der Parameter `multi_target_strategy=\"multi-output\"` ist eine spezifische Strategie, die für Multilabel-Klassifikationen verwendet wird, bei denen jedes Label als unabhängiger Klassifikationsausgang behandelt wird. Das bedeutet, dass das Modell für jeden Future Skill eine eigene, separate Vorhersage trifft, anstatt alle Labels als zusammenhängend zu betrachten. Diese Strategie ist besonders nützlich, wenn es keine Abhängigkeit zwischen den Labels gibt und jeder Future Skill unabhängig von den anderen erlernt oder vorhergesagt werden kann. Dadurch kann das Modell präziser und flexibler für komplexe Klassifikationsaufgaben mit mehreren Zielen arbeiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Laden des Modells\n",
    "model = SetFitModel.from_pretrained(\n",
    "    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", \n",
    "    multi_target_strategy=\"multi-output\",\n",
    "    labels=['Data Analytics & KI', 'Softwareentwicklung', 'Nutzerzentriertes Design', 'IT-Architektur', 'Hardware/Robotikentwicklung', 'Quantencomputing'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auswahl GPU/CPU\n",
    "\n",
    "Es wird zunächst geprüft, ob CUDA (und damit eine GPU) verfügbar ist, indem `torch.cuda.is_available()` aufgerufen wird. Wenn CUDA verfügbar ist, wird die GPU als Rechengerät (`device`) ausgewählt, und der Name der GPU mit `torch.cuda.get_device_name(0)` ausgegeben. Falls CUDA nicht verfügbar ist, wird stattdessen die CPU als Rechengerät festgelegt. Anschließend wird das Modell mit `model.to(device)` entweder auf die GPU oder die CPU verschoben, abhängig von der zuvor getroffenen Geräteauswahl, um die Berechnungen entsprechend durchzuführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA verfügbar: True\n",
      "GPU: NVIDIA GeForce RTX 4060 Ti\n"
     ]
    }
   ],
   "source": [
    "# Prüfen, ob CUDA (GPU) verfügbar ist\n",
    "print(\"CUDA verfügbar:\", torch.cuda.is_available())\n",
    "\n",
    "# Wenn CUDA verfügbar ist, die GPU verwenden\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Verwende CPU\")\n",
    "\n",
    "# model auf GPU verschieben\n",
    "\n",
    "    # Sicherstellen, dass das Modell auf der GPU ist, wenn verfügbar\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für den Fall dass eine GPU vorhanden ist: `PyTorch` wird verwendet, um die GPU für das Training von Modellen zu verwenden. `PyTorch` nutzt einen Speicher-Caching-Mechanismus, um GPU-Speicher effizient zu verwalten. Wenn Tensoren gelöscht oder nicht mehr benötigt werden, bleibt der Speicher oft noch im Cache, damit zukünftige Speicheranforderungen schneller bedient werden können. Der Befehl `torch.cuda.empty_cache()` leert im folgenden den Cache und gibt den belegten Speicher an das GPU-System zurück, ohne jedoch den aktuell in Verwendung befindlichen Speicher zu beeinflussen. Wird der Cache nicht regelmäßiig geleert, kann das einen ernormen Einfluss auf die Bearbeitungszeit des Trainings eines Classifiers haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der `SetFitTrainer` wird instanziiert, um das Modell auf den Trainings- und Evaluierungsdatensätzen zu trainieren. Eine Bewertung des Modells wird erst zu einem späteren Zeitpunkt möglich, wenn Testdaten vorliegen.\n",
    "\n",
    "1. Das vortrainierte `model` (siehe oben) wird als Grundlage für das Training verwendet, das auf dem Few-Shot Learning-Ansatz basiert.\n",
    "2. Der `train_dataset` enthält die Trainingsdaten, ein `eval_dataset` liegt derzeit nicht vor.\n",
    "3. Die Verlustfunktion (`loss_class`) wird als *Cosine Similarity Loss* festgelegt. Die *Cosine Similarity Loss* ist eine Verlustfunktion, die darauf basiert, wie ähnlich sich zwei Vektoren sind. In diesem Fall werden die Ausgabevektoren des Modells und die Zielvektoren (also die \"Labels\") mithilfe der Kosinus-Ähnlichkeit verglichen. Je höher die Ähnlichkeit also, desto besser das Modell. \n",
    "4. Die `column_mapping` gibt an, welche Spalten in den Datensätzen die Texte (`sentence` → `text`) und die zugehörigen Labels (`labels` → `label`) enthalten, sodass das Modell die Daten korrekt interpretieren kann.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hueck\\AppData\\Local\\Temp\\ipykernel_15832\\2679062307.py:1: DeprecationWarning: `SetFitTrainer` has been deprecated and will be removed in v2.0.0 of SetFit. Please use `Trainer` instead.\n",
      "  trainer = SetFitTrainer(\n",
      "Applying column mapping to the training dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ad434dbdb74fd6896506d6932870f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1577 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset, \n",
    "    #eval_dataset=eval_dataset,\n",
    "    loss_class=CosineSimilarityLoss,\n",
    "    num_iterations=40, \n",
    "    num_epochs=2,\n",
    "    column_mapping={\"sentence\": \"text\", \"labels\": \"label\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun kann das Training beginnen: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 126160\n",
      "  Batch size = 16\n",
      "  Num epochs = 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849972fec4c84c7cb7aa4c1f360e85c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15770 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.1571, 'grad_norm': 1.779844045639038, 'learning_rate': 1.2682308180088778e-08, 'epoch': 0.0}\n",
      "{'embedding_loss': 0.1986, 'grad_norm': 2.48179292678833, 'learning_rate': 6.341154090044388e-07, 'epoch': 0.01}\n",
      "{'embedding_loss': 0.1774, 'grad_norm': 1.4752390384674072, 'learning_rate': 1.2682308180088775e-06, 'epoch': 0.01}\n",
      "{'embedding_loss': 0.136, 'grad_norm': 1.5738091468811035, 'learning_rate': 1.9023462270133167e-06, 'epoch': 0.02}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hueck\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\future-skill-classification--ys6-NbF-py3.12\\Lib\\site-packages\\setfit\\trainer.py:518\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, args, trial, **kwargs)\u001b[0m\n\u001b[0;32m    513\u001b[0m train_parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_to_parameters(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset)\n\u001b[0;32m    514\u001b[0m full_parameters \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    515\u001b[0m     train_parameters \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_to_parameters(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset \u001b[38;5;28;01melse\u001b[39;00m train_parameters\n\u001b[0;32m    516\u001b[0m )\n\u001b[1;32m--> 518\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfull_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_classifier(\u001b[38;5;241m*\u001b[39mtrain_parameters, args\u001b[38;5;241m=\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\Hueck\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\future-skill-classification--ys6-NbF-py3.12\\Lib\\site-packages\\setfit\\trainer.py:569\u001b[0m, in \u001b[0;36mTrainer.train_embeddings\u001b[1;34m(self, x_train, y_train, x_eval, y_eval, args)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m    562\u001b[0m     losses\u001b[38;5;241m.\u001b[39mBatchAllTripletLoss,\n\u001b[0;32m    563\u001b[0m     losses\u001b[38;5;241m.\u001b[39mBatchHardTripletLoss,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    566\u001b[0m     SupConLoss,\n\u001b[0;32m    567\u001b[0m ):\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mst_trainer\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_sampler \u001b[38;5;241m=\u001b[39m BatchSamplers\u001b[38;5;241m.\u001b[39mGROUP_BY_LABEL\n\u001b[1;32m--> 569\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mst_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hueck\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\future-skill-classification--ys6-NbF-py3.12\\Lib\\site-packages\\transformers\\trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hueck\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\future-skill-classification--ys6-NbF-py3.12\\Lib\\site-packages\\transformers\\trainer.py:2345\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2342\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2344\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 2345\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   2346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_batched_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m   2348\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_num_input_tokens_seen\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hueck\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\future-skill-classification--ys6-NbF-py3.12\\Lib\\site-packages\\accelerate\\data_loader.py:559\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 559\u001b[0m         current_batch \u001b[38;5;241m=\u001b[39m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_non_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    560\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state_dict()\n\u001b[0;32m    561\u001b[0m     next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n",
      "File \u001b[1;32mc:\\Users\\Hueck\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\future-skill-classification--ys6-NbF-py3.12\\Lib\\site-packages\\accelerate\\utils\\operations.py:185\u001b[0m, in \u001b[0;36msend_to_device\u001b[1;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m         skip_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[0;32m    184\u001b[0m         {\n\u001b[1;32m--> 185\u001b[0m             k: t \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m skip_keys \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    187\u001b[0m         }\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[1;32mc:\\Users\\Hueck\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\future-skill-classification--ys6-NbF-py3.12\\Lib\\site-packages\\accelerate\\utils\\operations.py:156\u001b[0m, in \u001b[0;36msend_to_device\u001b[1;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[0;32m    154\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu:0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das nach-trainierte Modell wird in einem ersten Schritt lokal gespeichert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speichern des Modells auf deinem PC\n",
    "model.save_pretrained(\"models\")  # Ersetze den Pfad durch deinen gewünschten Speicherort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In einem weiteren Schritt wird das Modeel auf das Hugging-Face-Hub geladen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(\"Chernoffface/fs-setfit-multilable-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done. Das Training ist beendet 🤗."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry Future Skills Enviorment",
   "language": "python",
   "name": "poetry_fs_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
