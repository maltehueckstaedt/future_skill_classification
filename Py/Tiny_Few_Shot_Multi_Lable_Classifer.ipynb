{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny few shot classifer\n",
    "\n",
    "Im folgenden wird ein *few shot classifier* f√ºr *multilabels* im [SetFit-Frame](https://huggingface.co/blog/setfit) Format erzeugt. Die *multilabels*-Option ist deshalb von N√∂ten, weil durchaus mehrere Future Skills in einem Kurs vermittelt werden k√∂nnen.\n",
    "\n",
    "Gearbeitet wird mit `Python 3.12.3`, in einem eigens daf√ºr erzeugten Poetry Enviorment (`Poetry Future Skills Enviorment`). Das Enviorment wird als Kernel h√§ndisch in Jupyter Notebook geladen. \n",
    "\n",
    "Weitere Pakete k√∂nnen dem Enviorment mit `poetry add <paketname>` hinzugef√ºgt werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !poetry add platformdirs\n",
    "# !poetry add pyreadr\n",
    "# Abh√§nggkeiten anzeigen:\n",
    "# !poetry show --tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pakete laden\n",
    "\n",
    "In einem ersten Schritt werden die n√∂tigen Pakete geladen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset  # Zum Laden von Datens√§tzen aus der Hugging Face `datasets` Bibliothek\n",
    "from sentence_transformers.losses import CosineSimilarityLoss  # Verlustfunktion basierend auf der Kosinus√§hnlichkeit, f√ºr Aufgaben wie Text√§hnlichkeit\n",
    "\n",
    "from setfit import SetFitModel, SetFitTrainer  # Zum Laden des SetFit-Modells und Trainers, speziell f√ºr Few-Shot Learning mit Satztransformern\n",
    "import pandas as pd  # Pandas f√ºr Datenmanipulation und -analyse\n",
    "from sklearn.preprocessing import LabelEncoder  # Zum Encoden von Labels in numerische Werte f√ºr maschinelles Lernen\n",
    "import os  # F√ºr den Umgang mit Betriebssystemfunktionen (z.B. Dateipfade)\n",
    "from datasets import Dataset, DatasetDict  # Zum Erstellen und Verwalten von Datens√§tzen in Hugging Face `datasets`-Format\n",
    "from sklearn.model_selection import train_test_split  # Zum Aufteilen der Daten in Trainings- und Testsets\n",
    "\n",
    "from transformers import TrainingArguments  # Zum Festlegen von Trainingsparametern f√ºr Modelle in der Transformers-Bibliothek\n",
    "\n",
    "import torch  # Erm√∂glicht die GPU-Nutzung und effiziente Verarbeitung von Tensoren\n",
    "import numpy as np  # F√ºr numerische Berechnungen und Arbeiten mit Arrays\n",
    "\n",
    "import re  # Paket f√ºr regul√§re Ausdr√ºcke (Regular Expressions), zum Suchen, Ersetzen oder Validieren von Textmustern.\n",
    "import html  # Modul zum Umgang mit HTML, wie z.B. Entschl√ºsseln von HTML-Entities.\n",
    "from unidecode import unidecode  # Bibliothek zum Entfernen von diakritischen Zeichen (z.B. Akzente) und zum Normalisieren von Unicode-Zeichen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbeitsverzeichnis bestimmen\n",
    "\n",
    "Es wird weiterhin das Rootverzeichnis bestimmt um es sp√§ter √ºber relative Pfade Daten und Modelle laden zu k√∂nnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hueck\\OneDrive\\Dokumente\\GitHub\\future_skill_classification\\Py\n"
     ]
    }
   ],
   "source": [
    "# checke home-verzeichnis\n",
    "print(os.getcwd())\n",
    "# setze korrektes Home-Verzeichnis, falls das derzeitige nicht das korrekte ist.\n",
    "os.chdir('c:/Users/Hueck/OneDrive/Dokumente/GitHub/future_skill_classification/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainings- und Testdaten\n",
    "\n",
    "Von Yannic Hinrichs wurden per String-Match Trainingsdaten erzeugt, die um weitere Daten erg√§nzt wurden, die keine Future Skills enthalten. Die Daten wurden weiterhin durch Yannic Hinrichs h√§ndisch kontrolliert. Alternativ bestehen weiterhin die Daten, die Franziska Weber f√ºr das Training ihres Classifiers verwendet hat. Dieses werden derzeit (Stand 21.10.24) ebenfalls f√ºr das Training des folgenden Classfiers verwendet.\n",
    "\n",
    "### Daten laden\n",
    "\n",
    "Testdaten liegen zu jetzigen Zeitpunkt nicht vor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence', 'Data Analytics & KI', 'Softwareentwicklung', 'Nutzerzentriertes Design', 'IT-Architektur', 'Hardware/Robotikentwicklung', 'Quantencomputing', 'Digital Literacy', 'Digital Ethics', 'Digitale Kollaboration', 'Digital Learning', 'Agiles Arbeiten', 'L√∂sungsf√§higkeit', 'Kreativit√§t', 'Unternehmerisches Handeln & Eigeninitiative', 'Interkulturelle Kommunikation', 'Resilienz', 'Urteilsf√§higkeit', 'Innovationskompetenz', 'Missionsorientierung', 'Ver√§nderungskompetenz', 'Dialog- und Konfliktf√§higkeit'],\n",
      "    num_rows: 1577\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Laden der Trainingsdaten\n",
    "df = pd.read_excel('data/train_data_franziska.xlsx')\n",
    "\n",
    "df = (df\n",
    "      .assign(sentence=lambda x: x['sentence'].astype(str))  # sentence-Spalte als String deklarieren\n",
    "      .fillna(0)  # NAs durch 0 ersetzen\n",
    "      .replace([np.inf, -np.inf], 0)  # infs durch 0 ersetzen\n",
    "      .pipe(lambda x: x.assign(**{col: x[col].astype('int64') for col in x.select_dtypes(include='number').columns}))  # Numerische Variablen runden\n",
    "     )\n",
    "\n",
    "# F√ºr die weitere Verarbeitung werden die Daten ins Hugging Face Dataset-Format transformiert. \n",
    "\n",
    "# # /////////////////////////////////////////////////\n",
    "# # Hinweis: Das Dataset-Format der Hugging Face datasets-Bibliothek basiert auf Apache Arrow und ist speichereffizient, was schnelles Laden und Verarbeiten gro√üer Datens√§tze erm√∂glicht. Es integriert sich nahtlos in Machine-Learning-Workflows und unterst√ºtzt einfache Transformationen und Vorverarbeitung durch Methoden wie .map(). Zudem ist es flexibel und kompatibel mit Pandas DataFrames sowie g√§ngigen Frameworks wie PyTorch und TensorFlow.\n",
    "# #//////////////////////////////////////////////////\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten verarbeiten\n",
    "\n",
    "#### Feature Auswahl\n",
    "\n",
    "Nun werden die Features des Classifiers ausgew√§hlt. Diese werden von den Kurstitel und -beschreibungen (`sentence`) separiert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data Analytics & KI', 'Softwareentwicklung', 'Nutzerzentriertes Design', 'IT-Architektur', 'Hardware/Robotikentwicklung', 'Quantencomputing']\n"
     ]
    }
   ],
   "source": [
    "# Ziehe Column-Names als Liste\n",
    "features = dataset.column_names\n",
    "\n",
    "# Definiere Columns, die nicht vom classifier ber√ºcksichtigt werden\n",
    "to_remove = ['sentence','Digital Literacy', 'Digital Ethics', 'Digitale Kollaboration', 'Digital Learning', 'Agiles Arbeiten', 'L√∂sungsf√§higkeit', 'Kreativit√§t', 'Unternehmerisches Handeln & Eigeninitiative', 'Interkulturelle Kommunikation', 'Resilienz', 'Urteilsf√§higkeit', 'Innovationskompetenz', 'Missionsorientierung', 'Ver√§nderungskompetenz', 'Dialog- und Konfliktf√§higkeit'] \n",
    "\n",
    "# Schlie√üe die oben definierten Variablen aus der Feature-Liste aus\n",
    "filtered_features = [feature for feature in features if feature not in to_remove]\n",
    "\n",
    "features = filtered_features\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot-Encoding\n",
    "\n",
    "F√ºr das Trainieren des Classifiers wird im folgenden ein *One-Hot-Encoding*-Array der Features erzeugt. Dies ist deshalb n√∂tig, da im folgenden ein Modell f√ºr ein Multi-Label-Szenario trainiert werden soll: Ein Kursangebot kann den erwerb mehrerer Future Skills versprechen, etwa Softwareentwicklung **&** IT-Architektur. Das One-Hot-Encoding wird ben√∂tigt, um die Labels der Features in eine Form zu bringen, die das Classifier Modell von SetFit korrekt verarbeiten kann. Es stellt sicher, dass alle Klassen gleichwertig behandelt werden, und erm√∂glicht die Berechnung von √Ñhnlichkeiten verschiedener Kursinhalte.\n",
    "\n",
    "Um den *One-Hot-Encoding*-Array zu erzeugen definieren wir `encode_labels(record)`: Diese Funktion geht durch jede Zeile eines Data Frames und erstellt eine neue Spalte namens `labels`. In dieser Spalte werden die Werte aus den oben definierten \"features\" gesammelt und in eine Liste gepackt. Diese Liste enth√§lt Werte der Features.\n",
    "\n",
    "Beispiel: Mit Blick auf die Features 'Data Analytics & KI', 'Softwareentwicklung' und 'Nutzerzentriertes Design', k√∂nnte ein Zeilenvektor des Arrays eines Kurses der sich mit Data Analytics & KI / Softwareentwicklung besch√§ftigt so aussehen: `[1, 1, 0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hueck\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\future-skill-classification--ys6-NbF-py3.12\\Lib\\site-packages\\datasets\\utils\\_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\n",
      "  obj.co_lnotab,  # for < python 3.10 [not counted in args]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bcb5d6cd37f467c8aa63e4366a14eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1577 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generiere Funktion f√ºr One-Hot-Encoding f√ºr jede Zeile (record)\n",
    "def encode_labels(record):\n",
    "    return {\"labels\": [record[feature] for feature in features]}\n",
    "\n",
    "# Wende Funktion auf train_data an\n",
    "dataset = dataset.map(encode_labels)\n",
    "\n",
    "# Um die Daten √ºbersichtlich im Viewer zu betrachten: Umwandlung und Pandas Data Frame\n",
    "pd_dataset = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir kopieren als n√§chstes das `dataset`-Objekt in ein neues Objekt namens `train_data` und ziehen davon nochmals eine Kopie mit Pandas, um die Daten ein letztes Mal im Viewer zu pr√ºfen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset\n",
    "\n",
    "# check data with pandas\n",
    "pd_train_dataset = train_dataset.to_pandas() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelltraining\n",
    "\n",
    "### Vorbereitung des Modells\n",
    "\n",
    "Es wird das vortrainierte sentence-transformers-Modell `paraphrase-multilingual-MiniLM-L12-v2` aus der Hugging Face Bibliothek geladen. Durch die Angabe der Strategie `one-vs-rest` wird das Modell f√ºr eine Multi-Label-Klassifikation konfiguriert, bei der jede Klasse eines Kurses einzeln vorhergesagt wird. Auf diese Weise wird es m√∂glich, mehrere Skills pro Kurs zu identifizieren.\n",
    "\n",
    "Das Modell `paraphrase-multilingual-MiniLM-L12-v2` eignet sich gut zur Klassifikation von Future Skills, da es auf semantische √Ñhnlichkeit und Paraphrasierung trainiert wurde. Dadurch kann es unterschiedliche Formulierungen eines Skills als denselben Future Skill erkennen, was bei der Erfassung von Konzepten hilfreich ist, die oft unterschiedlich beschrieben werden. Seine Multilingualit√§t macht es zudem flexibel f√ºr Kursdaten in verschiedenen Sprachen. Dank der MiniLM-Architektur ist das Modell effizient und erm√∂glicht schnelle Vorhersagen, ohne dabei an Leistungsf√§higkeit zu verlieren. Diese Eigenschaften machen es besonders n√ºtzlich, um abstrakte oder neu formulierte Skills zuverl√§ssig zu klassifizieren.\n",
    "\n",
    "Der Parameter `multi_target_strategy=\"multi-output\"` ist eine spezifische Strategie, die f√ºr Multilabel-Klassifikationen verwendet wird, bei denen jedes Label als unabh√§ngiger Klassifikationsausgang behandelt wird. Das bedeutet, dass das Modell f√ºr jeden Future Skill eine eigene, separate Vorhersage trifft, anstatt alle Labels als zusammenh√§ngend zu betrachten. Diese Strategie ist besonders n√ºtzlich, wenn es keine Abh√§ngigkeit zwischen den Labels gibt und jeder Future Skill unabh√§ngig von den anderen erlernt oder vorhergesagt werden kann. Dadurch kann das Modell pr√§ziser und flexibler f√ºr komplexe Klassifikationsaufgaben mit mehreren Zielen arbeiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Laden des Modells\n",
    "model = SetFitModel.from_pretrained(\n",
    "    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", \n",
    "    multi_target_strategy=\"multi-output\",\n",
    "    labels=['Data Analytics & KI', 'Softwareentwicklung', 'Nutzerzentriertes Design', 'IT-Architektur', 'Hardware/Robotikentwicklung', 'Quantencomputing'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auswahl GPU/CPU\n",
    "\n",
    "Es wird zun√§chst gepr√ºft, ob CUDA (und damit eine GPU) verf√ºgbar ist, indem `torch.cuda.is_available()` aufgerufen wird. Wenn CUDA verf√ºgbar ist, wird die GPU als Rechenger√§t (`device`) ausgew√§hlt, und der Name der GPU mit `torch.cuda.get_device_name(0)` ausgegeben. Falls CUDA nicht verf√ºgbar ist, wird stattdessen die CPU als Rechenger√§t festgelegt. Anschlie√üend wird das Modell mit `model.to(device)` entweder auf die GPU oder die CPU verschoben, abh√§ngig von der zuvor getroffenen Ger√§teauswahl, um die Berechnungen entsprechend durchzuf√ºhren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA verf√ºgbar: True\n",
      "GPU: NVIDIA GeForce RTX 4060 Ti\n"
     ]
    }
   ],
   "source": [
    "# Pr√ºfen, ob CUDA (GPU) verf√ºgbar ist\n",
    "print(\"CUDA verf√ºgbar:\", torch.cuda.is_available())\n",
    "\n",
    "# Wenn CUDA verf√ºgbar ist, die GPU verwenden\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Verwende CPU\")\n",
    "\n",
    "# model auf GPU verschieben\n",
    "\n",
    "    # Sicherstellen, dass das Modell auf der GPU ist, wenn verf√ºgbar\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F√ºr den Fall dass eine GPU vorhanden ist: `PyTorch` wird verwendet, um die GPU f√ºr das Training von Modellen zu verwenden. `PyTorch` nutzt einen Speicher-Caching-Mechanismus, um GPU-Speicher effizient zu verwalten. Wenn Tensoren gel√∂scht oder nicht mehr ben√∂tigt werden, bleibt der Speicher oft noch im Cache, damit zuk√ºnftige Speicheranforderungen schneller bedient werden k√∂nnen. Der Befehl `torch.cuda.empty_cache()` leert im folgenden den Cache und gibt den belegten Speicher an das GPU-System zur√ºck, ohne jedoch den aktuell in Verwendung befindlichen Speicher zu beeinflussen. Wird der Cache nicht regelm√§√üiig geleert, kann das einen ernormen Einfluss auf die Bearbeitungszeit des Trainings eines Classifiers haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der `SetFitTrainer` wird instanziiert, um das Modell auf den Trainings- und Evaluierungsdatens√§tzen zu trainieren. Eine Bewertung des Modells wird erst zu einem sp√§teren Zeitpunkt m√∂glich, wenn Testdaten vorliegen.\n",
    "\n",
    "1. Das vortrainierte `model` (siehe oben) wird als Grundlage f√ºr das Training verwendet, das auf dem Few-Shot Learning-Ansatz basiert.\n",
    "2. Der `train_dataset` enth√§lt die Trainingsdaten, ein `eval_dataset` liegt derzeit nicht vor.\n",
    "3. Die Verlustfunktion (`loss_class`) wird als *Cosine Similarity Loss* festgelegt. Die *Cosine Similarity Loss* ist eine Verlustfunktion, die darauf basiert, wie √§hnlich sich zwei Vektoren sind. In diesem Fall werden die Ausgabevektoren des Modells und die Zielvektoren (also die \"Labels\") mithilfe der Kosinus-√Ñhnlichkeit verglichen. Je h√∂her die √Ñhnlichkeit also, desto besser das Modell. \n",
    "4. Die `column_mapping` gibt an, welche Spalten in den Datens√§tzen die Texte (`sentence` ‚Üí `text`) und die zugeh√∂rigen Labels (`labels` ‚Üí `label`) enthalten, sodass das Modell die Daten korrekt interpretieren kann.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hueck\\AppData\\Local\\Temp\\ipykernel_15832\\2679062307.py:1: DeprecationWarning: `SetFitTrainer` has been deprecated and will be removed in v2.0.0 of SetFit. Please use `Trainer` instead.\n",
      "  trainer = SetFitTrainer(\n",
      "Applying column mapping to the training dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ad434dbdb74fd6896506d6932870f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1577 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset, \n",
    "    #eval_dataset=eval_dataset,\n",
    "    loss_class=CosineSimilarityLoss,\n",
    "    num_iterations=40, \n",
    "    num_epochs=2,\n",
    "    column_mapping={\"sentence\": \"text\", \"labels\": \"label\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun kann das Training beginnen: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 126160\n",
      "  Batch size = 16\n",
      "  Num epochs = 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849972fec4c84c7cb7aa4c1f360e85c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15770 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.1571, 'grad_norm': 1.779844045639038, 'learning_rate': 1.2682308180088778e-08, 'epoch': 0.0}\n",
      "{'embedding_loss': 0.1986, 'grad_norm': 2.48179292678833, 'learning_rate': 6.341154090044388e-07, 'epoch': 0.01}\n",
      "{'embedding_loss': 0.1774, 'grad_norm': 1.4752390384674072, 'learning_rate': 1.2682308180088775e-06, 'epoch': 0.01}\n",
      "{'embedding_loss': 0.136, 'grad_norm': 1.5738091468811035, 'learning_rate': 1.9023462270133167e-06, 'epoch': 0.02}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hueck\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\future-skill-classification--ys6-NbF-py3.12\\Lib\\site-packages\\setfit\\trainer.py:518\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, args, trial, **kwargs)\u001b[0m\n\u001b[0;32m    513\u001b[0m train_parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_to_parameters(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset)\n\u001b[0;32m    514\u001b[0m full_parameters \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    515\u001b[0m     train_parameters \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_to_parameters(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset \u001b[38;5;28;01melse\u001b[39;00m train_parameters\n\u001b[0;32m    516\u001b[0m )\n\u001b[1;32m--> 518\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfull_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_classifier(\u001b[38;5;241m*\u001b[39mtrain_parameters, args\u001b[38;5;241m=\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\Hueck\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\future-skill-classification--ys6-NbF-py3.12\\Lib\\site-packages\\setfit\\trainer.py:569\u001b[0m, in \u001b[0;36mTrainer.train_embeddings\u001b[1;34m(self, x_train, y_train, x_eval, y_eval, args)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m    562\u001b[0m     losses\u001b[38;5;241m.\u001b[39mBatchAllTripletLoss,\n\u001b[0;32m    563\u001b[0m     losses\u001b[38;5;241m.\u001b[39mBatchHardTripletLoss,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    566\u001b[0m     SupConLoss,\n\u001b[0;32m    567\u001b[0m ):\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mst_trainer\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_sampler \u001b[38;5;241m=\u001b[39m BatchSamplers\u001b[38;5;241m.\u001b[39mGROUP_BY_LABEL\n\u001b[1;32m--> 569\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mst_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hueck\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\future-skill-classification--ys6-NbF-py3.12\\Lib\\site-packages\\transformers\\trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hueck\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\future-skill-classification--ys6-NbF-py3.12\\Lib\\site-packages\\transformers\\trainer.py:2345\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2342\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2344\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 2345\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   2346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_batched_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m   2348\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_num_input_tokens_seen\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hueck\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\future-skill-classification--ys6-NbF-py3.12\\Lib\\site-packages\\accelerate\\data_loader.py:559\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 559\u001b[0m         current_batch \u001b[38;5;241m=\u001b[39m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_non_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    560\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state_dict()\n\u001b[0;32m    561\u001b[0m     next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n",
      "File \u001b[1;32mc:\\Users\\Hueck\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\future-skill-classification--ys6-NbF-py3.12\\Lib\\site-packages\\accelerate\\utils\\operations.py:185\u001b[0m, in \u001b[0;36msend_to_device\u001b[1;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m         skip_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[0;32m    184\u001b[0m         {\n\u001b[1;32m--> 185\u001b[0m             k: t \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m skip_keys \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    187\u001b[0m         }\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[1;32mc:\\Users\\Hueck\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\future-skill-classification--ys6-NbF-py3.12\\Lib\\site-packages\\accelerate\\utils\\operations.py:156\u001b[0m, in \u001b[0;36msend_to_device\u001b[1;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[0;32m    154\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu:0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das nach-trainierte Modell wird in einem ersten Schritt lokal gespeichert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speichern des Modells auf deinem PC\n",
    "model.save_pretrained(\"models\")  # Ersetze den Pfad durch deinen gew√ºnschten Speicherort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In einem weiteren Schritt wird das Modeel auf das Hugging-Face-Hub geladen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(\"Chernoffface/fs-setfit-multilable-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done. Das Training ist beendet ü§ó."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry Future Skills Enviorment",
   "language": "python",
   "name": "poetry_fs_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
