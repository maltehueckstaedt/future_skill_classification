{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny few shot classifer\n",
    "\n",
    "Im folgenden wird ein few shot classifier im [SetFit-Frame](https://huggingface.co/blog/setfit) work erzeugt.\n",
    "\n",
    "## Vorteile von SetFit\n",
    "\n",
    "Die Verwendung von SetFit (Sentence Transformer Fine-tuning) hat mehrere Vorteile für die hiesige Anwendung:\n",
    "\n",
    "1. **Few-Shot-Learning**: SetFit wurde speziell entwickelt, um mit sehr wenigen Trainingsbeispielen pro Klasse zu arbeiten (z. B. 8 Beispiele). Das Modell erreicht dennoch eine hohe Genauigkeit, was es ideal für Anwendungsfälle mit begrenzten annotierten Daten macht.\n",
    "\n",
    "2. **Effiziente Fine-Tuning-Methode**: Anstatt ein großes Sprachmodell vollständig zu trainieren, verwendet SetFit einen zweistufigen Ansatz. Zuerst wird ein vortrainiertes Sentence Transformer-Modell (z. B. MiniLM) auf die Textdaten angepasst, dann wird ein einfacher Klassifikator darauf angewendet. Dies ist sowohl rechen- als auch speichereffizient.\n",
    "\n",
    "3. **Keine zusätzliche Tokenisierung**: SetFit arbeitet direkt mit Sentence Embeddings, was bedeutet, dass keine zusätzliche Tokenisierung oder Verarbeitung für die Fine-Tuning-Aufgabe erforderlich ist. Dies vereinfacht den Einsatz und spart Rechenressourcen.\n",
    "\n",
    "4. **Gute Performance bei kleinerem Modell**: Durch die Verwendung von kompakten Sentence Transformers kann SetFit eine hohe Genauigkeit erreichen, ohne die Ressourcenanforderungen eines größeren Modells zu benötigen.\n",
    "\n",
    "5. **Schnelle Anpassung**: Aufgrund des effizienten Fine-Tuning-Prozesses kann SetFit in kürzerer Zeit auf neue Datensätze angepasst werden, was es für den produktiven Einsatz in schnell wechselnden Umgebungen geeignet macht.\n",
    "\n",
    "# Berechnung des few shot Modells\n",
    "\n",
    "## Lade Bibliotheken\n",
    "\n",
    "Folgende Bibliotheken werden geladen:\n",
    "\n",
    "- `from datasets import load_dataset`: Zum Laden von Datensätzen aus der HuggingFace `datasets` Bibliothek, z.B. öffentliche oder lokale Datensätze.\n",
    "  \n",
    "- `from sentence_transformers.losses import CosineSimilarityLoss`: Wird verwendet, um einen *Cosine Similarity Loss* für das Training von sentence-transformers-Modellen zu definieren (z.B. bei Ähnlichkeitsvergleichen zwischen Textpaaren).\n",
    "  \n",
    "- `from setfit import SetFitModel, SetFitTrainer`:  \n",
    "  - **SetFitModel**: Ein Modell aus dem SetFit-Framework, das für Zero-Shot- oder Few-Shot-Learning verwendet wird.\n",
    "  - **SetFitTrainer**: Trainer-Klasse, die verwendet wird, um das SetFit-Modell auf spezifische Aufgaben zu trainieren.\n",
    "  \n",
    "- `import pandas as pd`: Für die Data-Wrangling\n",
    "  \n",
    "- `from sklearn.preprocessing import LabelEncoder`: Zum Kodieren von Zielvariablen (Labels) in numerische Form. Besonders nützlich, wenn die Labels in string-Format vorliegen und in Zahlen umgewandelt werden müssen, um sie einem Modell zu übergeben.\n",
    "  \n",
    "- `import os`: Ermöglicht die Interaktion mit dem Betriebssystem, wie z.B. den Zugriff auf Dateipfade, Erstellen von Ordnern oder Lesen von Umgebungsvariablen.\n",
    "  \n",
    "- `from datasets import Dataset, DatasetDict`:  \n",
    "  - **Dataset**: Klasse, um einen benutzerdefinierten Datensatz für die Verwendung mit der `datasets` Bibliothek zu erstellen.\n",
    "  - **DatasetDict**: Zum Organisieren und Verwalten mehrerer Datensätze (z.B. `train` und `test`).\n",
    "  \n",
    "- `from sklearn.model_selection import train_test_split`: Zum Aufteilen eines Datensatzes in Trainings- und Testdaten. Diese Methode wird oft verwendet, um Modelle zu validieren und zu evaluieren.\n",
    "  \n",
    "- `from transformers import TrainingArguments`: Legt die Trainingsparameter für Modelle aus der HuggingFace Transformers Bibliothek fest (z.B. Anzahl der Epochen, Batch-Größe, Optimierungsparameter etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "import torch #für gpu-nutzung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Daten\n",
    "### Lade Daten\n",
    "\n",
    "Um die Daten zu laden wird in einem ersten Schritt das Root-Verzeichnis ausgelesen und ggf. angepasst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hueck\\OneDrive\\Dokumente\\GitHub\\future_skill_classification\n"
     ]
    }
   ],
   "source": [
    "# checke home-verzeichnis\n",
    "print(os.getcwd())\n",
    "# setze home-verzeichnis\n",
    "os.chdir('c:/Users/Hueck/OneDrive/Dokumente/GitHub/future_skill_classification/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4060 Ti\n"
     ]
    }
   ],
   "source": [
    "# Select N examples per class (8 in this case)\n",
    "import torch\n",
    "print(torch.cuda.is_available())  # Sollte True ausgeben, wenn CUDA verfügbar ist\n",
    "print(torch.cuda.get_device_name(0))  # Zeigt den Namen deiner GPU an\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stimmt das Verzeichnis, kann im folgenden der Datensatz geladen werden. Wir laden die Daten und schauen uns die ersten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 637 entries, 0 to 636\n",
      "Data columns (total 3 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   titel_kursbeschreibung  637 non-null    object\n",
      " 1   label_num               637 non-null    int64 \n",
      " 2   label_text              637 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 15.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Laden Ihrer Excel-Datei\n",
    "df = pd.read_excel('data/fs_test_data.xlsx')\n",
    "df.head()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erzeuge Test- und Trainingsdaten\n",
    "\n",
    "Im folgenden wird der Datensatz für das Training und die Evaluierung des Clasifiers vorbereitet. \n",
    "\n",
    "1. Der Data Frame wird in zwei Teile geteilt: Trainingsdaten (train_df) und Testdaten (test_df). 20% der Daten werden für den Testdatensatz verwendet, während 80% als Trainingsdatensatz verwendet werden. Mit \n",
    "`random_state=42` wird sicher gestellt, dass die Aufteilung reproduzierbar ist. \n",
    "`stratify=df['label']` sichert schließlich, dass die Verteilung der Zielvariable (label) im Training- und Testdatensatz bleibt, wie im Ursprungsdatensatz `df`.\n",
    "\n",
    "2. Dataset.from_pandas():\n",
    "train_df.reset_index(drop=True) und test_df.reset_index(drop=True): Zuerst wird der Index des DataFrames zurückgesetzt, damit keine alte Indexspalte übernommen wird. Das drop=True sorgt dafür, dass der alte Index vollständig entfernt wird.\n",
    "Dataset.from_pandas(): Diese Methode konvertiert den Pandas DataFrame (train_df und test_df) in ein Dataset-Objekt aus der HuggingFace datasets-Bibliothek, das besser geeignet ist für den Einsatz in NLP-Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Aufteilen des DataFrames\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label_num'])\n",
    "\n",
    "# Konvertieren in Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "# Erstellen eines DatasetDicts\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modell und Trainer initialisieren\n",
    "\n",
    "Zunächst werden die Klassen `SetFitModel` und `SetFitTrainer` aus der `SetFit`-Bibliothek importiert. `SetFit` basiert auf der sentence-transformers-Architektur und ist speziell für effiziente Few-Shot-Klassifikationsaufgaben konzipiert.\n",
    "\n",
    "Mit  `paraphrase-mpnet-base-v2` wird ein vortrainiertes Modell geladen, das auf der MPNet-Architektur basiert und darauf spezialisiert ist, semantische Ähnlichkeiten zwischen Texten zu erkennen. Die MPNet-Architektur ist ein Sprachmodell, das darauf trainiert wird, den Kontext und die Bedeutung von Wörtern in einem Satz besser zu verstehen. Es maskiert (versteckt) einige Wörter und versucht, diese anhand des restlichen Satzes vorherzusagen, um so die Bedeutung zu erfassen. Zusätzlich permutiert (vertauscht) es die Reihenfolge der Wörter, um die Wechselwirkungen zwischen ihnen zu lernen. Dadurch kann MPNet komplexe Satzstrukturen und den Sinn von Texten sehr präzise analysieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from setfit import SetFitModel, SetFitTrainer\n",
    "model = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes wird getestet, ob eine GPU (CUDA) vorhanden ist. Ist dies der Fall, wird das Modell mit Hilfe der GPU berechnet. Weiterhin werden folgende Parameter spezifiziert:\n",
    "\n",
    "- **`SetFitTrainer`:** Dies ist ein Trainer-Objekt, das den gesamten Trainingsprozess steuert, einschließlich der Festlegung der Trainingsdaten, der Verlustfunktion und der Trainingsparameter.\n",
    "\n",
    "- **`Modell`:** Das Modell, das trainiert wird, ist ein vortrainiertes Sprachmodell, das an die aktuelle Aufgabe angepasst wird.\n",
    "\n",
    "- **`Trainings- und Testdatensatz`:** `train_dataset` und `eval_dataset` sind die Datensätze, die zum Training und zur Evaluierung des Modells verwendet werden. Der Trainingsdatensatz dient dem Lernen, während der Testdatensatz die Modellleistung prüft.\n",
    "\n",
    "- **`CosineSimilarityLoss`:** Eine Verlustfunktion, die den Kosinus-Abstand zwischen Satzpaaren misst, um die Ähnlichkeit zwischen ihnen zu bewerten. Sie hilft dem Modell, besser darin zu werden, ähnliche Texte zu erkennen und zu klassifizieren.\n",
    "\n",
    "- **`batch_size`:** Die Anzahl der Beispiele, die das Modell in einem Durchlauf verarbeitet, bevor es die Modellparameter aktualisiert. Größere Batches können zu stabileren Aktualisierungen führen, benötigen jedoch mehr Speicher.\n",
    "\n",
    "- **`num_iterations`:** Die Anzahl der Textpaarungen, die pro Beispiel während des Trainings erstellt werden. Mehr Iterationen können zu einer besseren Generalisierung des Modells führen.\n",
    "\n",
    "- **`num_epochs`:** Die Anzahl der kompletten Durchläufe durch den gesamten Trainingsdatensatz. Mehr Epochen können zu einer verbesserten Modellleistung führen, erhöhen jedoch auch die Trainingszeit.\n",
    "\n",
    "- **`column_mapping`:** Hier wird festgelegt, welche Spalten des Datensatzes als Texte (`\"titel_kursbeschreibung\"`) und als Labels (`\"label_num\"`) verwendet werden sollen. Dies ist wichtig, um dem Modell die Struktur des Datensatzes verständlich zu machen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hueck\\AppData\\Local\\Temp\\ipykernel_15616\\3209044073.py:13: DeprecationWarning: `SetFitTrainer` has been deprecated and will be removed in v2.0.0 of SetFit. Please use `Trainer` instead.\n",
      "  trainer = SetFitTrainer(\n",
      "Applying column mapping to the training dataset\n",
      "Applying column mapping to the evaluation dataset\n",
      "c:\\Users\\Hueck\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\utils\\_dill.py:379: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\n",
      "  obj.co_lnotab,  # for < python 3.10 [not counted in args]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709c90f84bf84383a3366ca74ca3aced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/509 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sicherstellen, dass CUDA verfügbar ist und Gerät festlegen\n",
    "training_device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# Modell auf die GPU verschieben\n",
    "model = model.to(training_device)\n",
    "\n",
    "# Initialisieren des Trainers mit den Trainingsargumenten\n",
    "trainer = SetFitTrainer(\n",
    "    model=model, \n",
    "    train_dataset=dataset_dict['train'],\n",
    "    eval_dataset=dataset_dict['test'],\n",
    "    loss_class=CosineSimilarityLoss,\n",
    "    batch_size=16,\n",
    "    num_iterations=10,  # Anzahl der Textpaarungen pro Beispiel\n",
    "    num_epochs=1,        # Anzahl der Epochen\n",
    "    column_mapping={\"titel_kursbeschreibung\": \"text\", \"label_num\": \"label\"}\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun wird das Training induziert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 10180\n",
      "  Batch size = 16\n",
      "  Num epochs = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f7c50f7b9d4c54b4cafc101351ec03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/637 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.2519, 'grad_norm': 2.2817471027374268, 'learning_rate': 3.125e-07, 'epoch': 0.0}\n",
      "{'embedding_loss': 0.2065, 'grad_norm': 1.211120843887329, 'learning_rate': 1.5625e-05, 'epoch': 0.08}\n",
      "{'embedding_loss': 0.1206, 'grad_norm': 2.5888679027557373, 'learning_rate': 1.87434554973822e-05, 'epoch': 0.16}\n",
      "{'embedding_loss': 0.062, 'grad_norm': 1.6714777946472168, 'learning_rate': 1.699825479930192e-05, 'epoch': 0.24}\n",
      "{'embedding_loss': 0.0311, 'grad_norm': 0.9450912475585938, 'learning_rate': 1.5253054101221642e-05, 'epoch': 0.31}\n",
      "{'embedding_loss': 0.0124, 'grad_norm': 1.5740643739700317, 'learning_rate': 1.3507853403141362e-05, 'epoch': 0.39}\n",
      "{'embedding_loss': 0.0085, 'grad_norm': 0.0821307972073555, 'learning_rate': 1.1762652705061085e-05, 'epoch': 0.47}\n",
      "{'embedding_loss': 0.0022, 'grad_norm': 0.053271178156137466, 'learning_rate': 1.0017452006980804e-05, 'epoch': 0.55}\n",
      "{'embedding_loss': 0.0043, 'grad_norm': 0.10809438675642014, 'learning_rate': 8.272251308900523e-06, 'epoch': 0.63}\n",
      "{'embedding_loss': 0.001, 'grad_norm': 0.055178798735141754, 'learning_rate': 6.527050610820245e-06, 'epoch': 0.71}\n",
      "{'embedding_loss': 0.0012, 'grad_norm': 1.0906651020050049, 'learning_rate': 4.781849912739966e-06, 'epoch': 0.78}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b55d21497244f4899008ec7712081a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/5 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0009, 'grad_norm': 0.3505357801914215, 'learning_rate': 3.036649214659686e-06, 'epoch': 0.86}\n",
      "{'embedding_loss': 0.0012, 'grad_norm': 0.030179228633642197, 'learning_rate': 1.2914485165794066e-06, 'epoch': 0.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a16678685b47139912e52fc2c763dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/5 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 14351.2202, 'train_samples_per_second': 0.709, 'train_steps_per_second': 0.044, 'train_loss': 0.03563860492134581, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluieren des Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8828125}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "metrics = trainer.evaluate()\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload des Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755b9200896e461cb934236e7982355e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23f227bbf0c4ed7b81d4d5218c3703c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7ae5d1d74640949c9ea8f6e04fcd73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_head.pkl:   0%|          | 0.00/37.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Chernoffface/fs-setfit-model/commit/d3ad3bd17a0cd533895fae3c22adc590d1deeea1', commit_message='Add SetFit model', commit_description='', oid='d3ad3bd17a0cd533895fae3c22adc590d1deeea1', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(\"Chernoffface/fs-setfit-model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
